from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain_community.llms import LlamaCpp
from langchain_core.output_parsers import StrOutputParser

import requests
import json
import asyncio
import time
import socket

class HCSInsuranceAssistant:
    def __init__(self, model_path: str):
        self.lock = asyncio.Lock()

        # System prompts
        self.system_prompt = SystemMessagePromptTemplate.from_template(
            "Je bent een assistent voor HCS-Company autoverzekeringen. Beantwoord klantvragen over autoverzekeringen zonder extra labels zoals System: of AI:. Geef directe antwoorden op basis van de context en vraag om verduidelijking als dat nodig is. Antwoorden moeten altijd in het Nederlands zijn."
            "Context: {context}"
        )

        self.system_prompt_without_RAG = SystemMessagePromptTemplate.from_template(
            "Je bent een assistent voor HCS-Company autoverzekeringen. Beantwoord klantvragen over autoverzekeringen zonder extra labels zoals System: of AI:. Geef directe antwoorden en vraag om verduidelijking als dat nodig is. Antwoorden moeten altijd in het Nederlands zijn."
        )

        # Human prompt
        self.human_prompt = HumanMessagePromptTemplate.from_template(
            "Vraag: {question} "
            "Antwoord: "
        )

        # Chat template with RAG
        self.prompt = ChatPromptTemplate.from_messages(
            [
                self.system_prompt,
                self.human_prompt
            ]
        )

        # Chat template without RAG
        self.prompt_without_RAG = ChatPromptTemplate.from_messages(
            [
                self.system_prompt_without_RAG,
                self.human_prompt
            ]
        )

        # AI Model
        self.llm = LlamaCpp(
            model_path=model_path,
            max_tokens=200,         # max tokens: decides the maximum number of tokens that can be generated by the model
            n_ctx=2048,             # context length: decides the maximum number of tokens that can be processed by the model
            temperature=0.1,        # temperature: controls the creativity of the model
            n_gpu_layers=1024,      # number of layers the GPU uses
        )

        # Parser
        parser = StrOutputParser()

        # Chains
        self.chain_with_RAG = self.prompt | self.llm | parser
        self.chain_without_RAG = self.prompt_without_RAG | self.llm | parser

    async def generate_response_with_RAG(self, request: object):
        retrieved_docs = self.get_relevant_documents(request)
        print("Retrieved docs: ", json.dumps(retrieved_docs, indent=4))

        answer = ""    
        # Stream response with context
        async with self.lock:  # This ensures only one request accesses the function at a time
            duration = time.time()
            async for chunk in self.chain_with_RAG.astream(input = { "context": retrieved_docs, "question": request.prompt }):
                yield chunk
                answer += chunk
                print("Chunk: ", chunk)
                await asyncio.sleep(0) # Sleep to allow other tasks to run

        print("Answer: ", answer)
        requests.post(
            "https://logger-coen-de-vries-dev.apps.sandbox-m4.g2pi.p1.openshiftapps.com/llm_logs",
            json={
                "correlation_id": request.correlation_id,
                "with_rag_answer": answer,
                "with_rag_duration": time.time() - duration,
                "url": socket.gethostbyname(socket.gethostname())
            }
        )
        return

    async def generate_response_without_RAG(self, request: object) -> object:
        # Stream response without context
        answer = ""
        async with self.lock:  # This ensures only one request accesses the function at a time
            duration = time.time()
            async for chunk in self.chain_without_RAG.astream(input = { "question": request.prompt }):
                yield chunk
                answer += chunk
                print("Chunk: ", chunk)
                await asyncio.sleep(0) # Sleep to allow other tasks to run

        print("Answer: ", answer)
        requests.post(
            "https://logger-coen-de-vries-dev.apps.sandbox-m4.g2pi.p1.openshiftapps.com/llm_logs",
            json={
                "correlation_id": request.correlation_id,
                "without_rag_answer": answer,
                "without_rag_duration": time.time() - duration,
                "url": socket.gethostbyname(socket.gethostname())
            }
        )
        return

    def get_relevant_documents(self, request: object):
        retrieved_docs = requests.post(
            "https://hcs-backend-coen-de-vries-dev.apps.sandbox-m4.g2pi.p1.openshiftapps.com/insurance_policies/similar",
            json={
                "text": request.prompt,
                "correlation_id": request.correlation_id
                }
        )

        if retrieved_docs.json() != []:
            formatted_docs = []
            for doc in retrieved_docs.json():
                formatted_docs.append(
                    doc["title"] + " " + doc["content"]
                )

            return formatted_docs
        return ""