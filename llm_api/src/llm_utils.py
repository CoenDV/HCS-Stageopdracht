import requests
import json
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain_community.llms import LlamaCpp
from langchain_core.output_parsers import StrOutputParser
import asyncio

class HCSInsuranceAssistant:
    def __init__(self, model_path: str):
        self.lock = asyncio.Lock()

        # System prompts
        self.system_prompt = SystemMessagePromptTemplate.from_template(
            "Je bent een assistent voor HCS-Company autoverzekeringen. Beantwoord klantvragen over autoverzekeringen zonder extra labels zoals System: of AI:. Geef directe antwoorden op basis van de context en vraag om verduidelijking als dat nodig is. Antwoorden moeten altijd in het Nederlands zijn."
            "Context: {context}"
        )

        self.system_prompt_without_RAG = SystemMessagePromptTemplate.from_template(
            "Je bent een assistent voor HCS-Company autoverzekeringen. Beantwoord klantvragen over autoverzekeringen zonder extra labels zoals System: of AI:. Geef directe antwoorden en vraag om verduidelijking als dat nodig is. Antwoorden moeten altijd in het Nederlands zijn."
        )

        # Human prompt
        self.human_prompt = HumanMessagePromptTemplate.from_template(
            "Vraag: {question} "
            "Antwoord: "
        )

        # Chat template with RAG
        self.prompt = ChatPromptTemplate.from_messages(
            [
                self.system_prompt,
                self.human_prompt
            ]
        )

        # Chat template without RAG
        self.prompt_without_RAG = ChatPromptTemplate.from_messages(
            [
                self.system_prompt_without_RAG,
                self.human_prompt
            ]
        )

        # AI Model
        self.llm = LlamaCpp(
            model_path=model_path,
            max_tokens=200,         # max tokens: decides the maximum number of tokens that can be generated by the model
            n_ctx=2048,             # context length: decides the maximum number of tokens that can be processed by the model
            temperature=0.1,        # temperature: controls the creativity of the model
            n_gpu_layers=1024,      # number of layers the GPU uses
        )

        # Parser
        parser = StrOutputParser()

        # Chains
        self.chain_with_RAG = self.prompt | self.llm | parser
        self.chain_without_RAG = self.prompt_without_RAG | self.llm | parser

    async def generate_response_with_RAG(self, question: str) -> str:
        retrieved_docs = self.get_relevant_documents(question)
        print("Retrieved docs: ", json.dumps(retrieved_docs, indent=4))
        
        # Stream response with context
        async with self.lock:  # This ensures only one request accesses the function at a time
            async for chunk in self.chain_with_RAG.astream(input = { "context": retrieved_docs.content, "question": question }):
                yield chunk
                print("Chunk: ", chunk)
                await asyncio.sleep(0) # Sleep to allow other tasks to run
        return

    async def generate_response_without_RAG(self, question: str) -> str:
        # Stream response without context
        async with self.lock:  # This ensures only one request accesses the function at a time
            async for chunk in self.chain_without_RAG.astream(input = { "question": question }):
                yield chunk
                print("Chunk: ", chunk)
                await asyncio.sleep(0) # Sleep to allow other tasks to run
        return

    def get_relevant_documents(self, prompt: str):
        retrieved_docs = requests.post(
            "http://localhost:5000/insurance_policies/similar",
            json={"text": prompt}
        )

        if retrieved_docs.json() != [[]]:
            formatted_docs = []
            for doc in retrieved_docs.json():
                formatted_docs.append(
                    {
                        "title": doc["title"],
                        "content": doc["content"]
                    }
                )

            return formatted_docs
        return ""